{"category_type":"topic","category_name":"evaluation","repos_data":[{"full_name":"promptfoo/promptfoo","description":"Test your prompts, models, RAGs. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality. LLM evals for OpenAI/Azure GPT, Anthropic Claude, VertexAI Gemini, Ollama, Local & private models like Mistral/Mixtral/Llama with CI/CD","topics":["llm","prompt-engineering","prompts","llmops","prompt-testing","testing","rag","evaluation","evaluation-framework","llm-eval"],"created_at":"2023-04-28T15:48:49Z","pushed_at":"2024-01-20T00:45:30Z","stargazers_count":1748,"language":"TypeScript"},{"full_name":"PaesslerAG/gval","description":"Expression evaluation in golang","topics":["evaluate-expressions","expression-evaluator","gval","expression-language","parsing","godoc","evaluation","golang","parser","go"],"created_at":"2017-09-27T08:32:49Z","pushed_at":"2023-12-24T17:06:56Z","stargazers_count":685,"language":"Go"},{"full_name":"nullne/evaluator","description":"","topics":["golang","s-expressions","expression","evaluation","evaluator"],"created_at":"2017-04-27T18:31:46Z","pushed_at":"2023-04-14T11:05:55Z","stargazers_count":38,"language":"Go"}],"frecuent_topics":{"evaluation":3,"golang":2,"llm":1,"prompt-engineering":1,"prompts":1}}