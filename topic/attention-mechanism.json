{"category_type":"topic","category_name":"attention-mechanism","repos_data":[{"full_name":"BlinkDL/RWKV-LM","description":"RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.","topics":["attention-mechanism","deep-learning","gpt","gpt-2","gpt-3","language-model","linear-attention","lstm","pytorch","rnn"],"created_at":"2021-08-08T06:05:27Z","pushed_at":"2023-12-28T06:53:30Z","stargazers_count":10645,"language":"Python"},{"full_name":"lucidrains/reformer-pytorch","description":"Reformer, the efficient Transformer, in Pytorch","topics":["artificial-intelligence","transformers","attention-mechanism","machine-learning","pytorch"],"created_at":"2020-01-09T20:42:37Z","pushed_at":"2023-06-21T14:17:49Z","stargazers_count":2012,"language":"Python"},{"full_name":"kaushalshetty/Structured-Self-Attention","description":"A Structured Self-attentive Sentence Embedding","topics":["attention-mechanism","attention-model","self-attention","self-attentive-rnn","pytorch","deep-learning","python3","attention","visualization","classification"],"created_at":"2018-02-01T07:38:55Z","pushed_at":"2019-09-22T21:00:24Z","stargazers_count":494,"language":"Python"}],"frecuent_topics":{"attention-mechanism":3,"pytorch":3,"deep-learning":2,"gpt":1,"gpt-2":1}}