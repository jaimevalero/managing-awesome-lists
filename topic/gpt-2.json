{"category_type":"topic","category_name":"gpt-2","repos_data":[{"full_name":"BlinkDL/RWKV-LM","description":"RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.","topics":["attention-mechanism","deep-learning","gpt","gpt-2","gpt-3","language-model","linear-attention","lstm","pytorch","rnn"],"created_at":"2021-08-08T06:05:27Z","pushed_at":"2023-12-28T06:53:30Z","stargazers_count":10645,"language":"Python"},{"full_name":"EleutherAI/gpt-neo","description":"An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.","topics":["language-model","transformers","gpt","gpt-2","gpt-3"],"created_at":"2020-07-05T10:23:46Z","pushed_at":"2022-02-25T06:27:12Z","stargazers_count":8070,"language":"Python"},{"full_name":"microsoft/LoRA","description":"Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"","topics":["gpt-2","adaptation","language-model","gpt-3","low-rank","pytorch","deep-learning","roberta","deberta","lora"],"created_at":"2021-06-18T02:16:35Z","pushed_at":"2024-01-09T15:03:19Z","stargazers_count":7847,"language":"Python"},{"full_name":"graykode/gpt-2-Pytorch","description":"Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation","topics":["gpt-2","pytorch","implementation","nlp","text-generator","story-telling","gpt2","natural-language-processing"],"created_at":"2019-02-18T08:06:33Z","pushed_at":"2019-07-08T15:24:35Z","stargazers_count":908,"language":"Python"}],"frecuent_topics":{"gpt-2":4,"gpt-3":3,"language-model":3,"pytorch":3,"deep-learning":2}}