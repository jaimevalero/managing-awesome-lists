{"category_type":"topic","category_name":"streamingllm","repos_data":[{"full_name":"DefTruth/Awesome-LLM-Inference","description":"ðŸ“–A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.","topics":["flash-attention","flash-attention-2","paged-attention","streaming-llm","streamingllm","flash-decoding","tensorrt-llm","mamba","vllm","awq"],"created_at":"2023-08-27T02:32:15Z","pushed_at":"2024-01-13T02:58:31Z","stargazers_count":654,"language":"unknown"}],"frecuent_topics":{"flash-attention":1,"flash-attention-2":1,"paged-attention":1,"streaming-llm":1,"streamingllm":1}}