{"category_type":"topic","category_name":"pipeline-parallelism","repos_data":[{"full_name":"hpcaitech/ColossalAI","description":"Making large AI models cheaper, faster and more accessible","topics":["deep-learning","hpc","large-scale","data-parallelism","pipeline-parallelism","model-parallelism","ai","big-model","distributed-computing","inference"],"created_at":"2021-10-28T16:19:44Z","pushed_at":"2024-01-13T04:48:09Z","stargazers_count":36094,"language":"Python"},{"full_name":"microsoft/DeepSpeed","description":"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","topics":["deep-learning","pytorch","gpu","machine-learning","billion-parameters","data-parallelism","model-parallelism","inference","pipeline-parallelism","compression"],"created_at":"2020-01-23T18:35:18Z","pushed_at":"2024-01-13T07:20:25Z","stargazers_count":30732,"language":"Python"},{"full_name":"kakaobrain/torchgpipe","description":"A GPipe implementation in PyTorch","topics":["deep-learning","pytorch","gpipe","model-parallelism","pipeline-parallelism","parallelism","checkpointing"],"created_at":"2019-05-10T10:25:41Z","pushed_at":"2020-09-18T14:00:28Z","stargazers_count":762,"language":"Python"}],"frecuent_topics":{"deep-learning":3,"pipeline-parallelism":3,"model-parallelism":3,"data-parallelism":2,"inference":2}}