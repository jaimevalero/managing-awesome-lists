{"category_type":"topic","category_name":"language-model","repos_data":[{"full_name":"huggingface/transformers","description":"ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.","topics":["nlp","natural-language-processing","pytorch","language-model","tensorflow","bert","language-models","pytorch-transformers","nlp-library","transformer"],"created_at":"2018-10-29T13:56:00Z","pushed_at":"2024-01-21T09:20:13Z","stargazers_count":118997,"language":"Python"},{"full_name":"dair-ai/Prompt-Engineering-Guide","description":"üêô Guides, papers, lecture, notebooks and resources for prompt engineering","topics":["deep-learning","prompt-engineering","openai","chatgpt","language-model"],"created_at":"2022-12-16T16:04:50Z","pushed_at":"2024-01-12T05:00:25Z","stargazers_count":39275,"language":"Jupyter Notebook"},{"full_name":"LAION-AI/Open-Assistant","description":"OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.","topics":["chatgpt","language-model","rlhf","ai","assistant","discord-bot","machine-learning","nextjs","python"],"created_at":"2022-12-13T05:24:17Z","pushed_at":"2024-01-06T18:47:41Z","stargazers_count":36110,"language":"Python"},{"full_name":"tatsu-lab/stanford_alpaca","description":"Code and documentation to train Stanford's Alpaca models, and generate the data.","topics":["deep-learning","instruction-following","language-model"],"created_at":"2023-03-10T23:33:09Z","pushed_at":"2023-06-07T06:54:43Z","stargazers_count":28052,"language":"Python"},{"full_name":"deepset-ai/haystack","description":":mag: LLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","topics":["nlp","question-answering","bert","language-model","pytorch","semantic-search","squad","information-retrieval","summarization","transformers"],"created_at":"2019-11-14T09:05:28Z","pushed_at":"2024-01-12T18:04:09Z","stargazers_count":12314,"language":"Python"},{"full_name":"BlinkDL/RWKV-LM","description":"RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.","topics":["attention-mechanism","deep-learning","gpt","gpt-2","gpt-3","language-model","linear-attention","lstm","pytorch","rnn"],"created_at":"2021-08-08T06:05:27Z","pushed_at":"2023-12-28T06:53:30Z","stargazers_count":10645,"language":"Python"},{"full_name":"BlinkDL/ChatRWKV","description":"ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.","topics":["chatbot","chatgpt","language-model","pytorch","rnn","rwkv"],"created_at":"2023-01-13T08:07:40Z","pushed_at":"2023-12-27T03:33:12Z","stargazers_count":9011,"language":"Python"},{"full_name":"EleutherAI/gpt-neo","description":"An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.","topics":["language-model","transformers","gpt","gpt-2","gpt-3"],"created_at":"2020-07-05T10:23:46Z","pushed_at":"2022-02-25T06:27:12Z","stargazers_count":8070,"language":"Python"},{"full_name":"microsoft/LoRA","description":"Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"","topics":["gpt-2","adaptation","language-model","gpt-3","low-rank","pytorch","deep-learning","roberta","deberta","lora"],"created_at":"2021-06-18T02:16:35Z","pushed_at":"2024-01-09T15:03:19Z","stargazers_count":7847,"language":"Python"},{"full_name":"OpenNMT/OpenNMT-py","description":"Open Source Neural Machine Translation and (Large) Language Models in PyTorch","topics":["deep-learning","pytorch","machine-translation","neural-machine-translation","language-model","llms"],"created_at":"2017-02-22T19:01:50Z","pushed_at":"2024-01-12T13:07:11Z","stargazers_count":6433,"language":"Python"},{"full_name":"EleutherAI/gpt-neox","description":"An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.","topics":["deepspeed-library","gpt-3","transformers","language-model"],"created_at":"2020-12-22T14:37:54Z","pushed_at":"2024-01-13T00:07:52Z","stargazers_count":6318,"language":"Python"},{"full_name":"codertimo/BERT-pytorch","description":"Google AI 2018 BERT pytorch implementation","topics":["bert","transformer","pytorch","nlp","language-model"],"created_at":"2018-10-15T12:58:15Z","pushed_at":"2023-09-15T12:57:08Z","stargazers_count":5831,"language":"Python"},{"full_name":"mlfoundations/open_flamingo","description":"An open-source framework for training large multimodal models.","topics":["computer-vision","deep-learning","in-context-learning","language-model","multimodal-learning","pytorch","flamingo"],"created_at":"2022-10-20T00:32:35Z","pushed_at":"2023-12-02T04:19:32Z","stargazers_count":3236,"language":"Python"},{"full_name":"huggingface/pytorch-openai-transformer-lm","description":"üê•A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI","topics":["neural-networks","pytorch","openai","language-model","transformer"],"created_at":"2018-06-13T14:02:41Z","pushed_at":"2021-08-09T16:17:12Z","stargazers_count":1476,"language":"Python"},{"full_name":"langroid/langroid","description":"Harness LLMs with Multi-Agent Programming","topics":["agents","chatgpt","gpt","gpt-4","gpt4","language-model","llm","llm-agent","multi-agent-systems","openai-api"],"created_at":"2023-04-16T20:47:28Z","pushed_at":"2024-01-12T21:44:27Z","stargazers_count":913,"language":"Python"},{"full_name":"zjunlp/KnowLM","description":"An Open-sourced Knowledgable Large Language Model Framework.","topics":["llama","large-language-models","pre-trained-language-models","language-model","instruction-following","deep-learning","chinese","english","instructions","models"],"created_at":"2023-04-01T03:45:31Z","pushed_at":"2024-01-19T05:20:33Z","stargazers_count":866,"language":"Python"},{"full_name":"LiyuanLucasLiu/LM-LSTM-CRF","description":"Empower Sequence Labeling with Task-Aware Language Model","topics":["ner","language-model","crf","sequence-labeling","pytorch"],"created_at":"2017-09-12T19:24:49Z","pushed_at":"2022-06-22T20:29:39Z","stargazers_count":845,"language":"Python"},{"full_name":"salesforce/xgen","description":"Salesforce open-source LLMs with 8k sequence length.","topics":["llm","language-model","large-language-models","nlp"],"created_at":"2023-06-23T01:55:52Z","pushed_at":"2023-12-20T21:09:56Z","stargazers_count":695,"language":"Python"},{"full_name":"llm-jp/awesome-japanese-llm","description":"Êó•Êú¨Ë™ûLLM„Åæ„Å®„ÇÅ - Overview of Japanese LLMs","topics":["awesome","awesome-list","language-model","language-models","large-language-model","large-language-models","llm","llms","japanese","japanese-language"],"created_at":"2023-07-09T04:36:38Z","pushed_at":"2024-01-09T02:59:49Z","stargazers_count":561,"language":"unknown"},{"full_name":"declare-lab/flan-alpaca","description":"This repository contains code for extending the Stanford Alpaca synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.","topics":["flan-t5","alpaca","language-model","llm","transformers"],"created_at":"2023-03-22T11:35:19Z","pushed_at":"2023-07-04T23:49:23Z","stargazers_count":331,"language":"Python"},{"full_name":"Stonesjtu/Pytorch-NCE","description":"The Noise Contrastive Estimation for softmax output written in Pytorch","topics":["pytorch","nce","language-model","nce-criterion","importance-sampling","speedup","softmax"],"created_at":"2017-05-19T01:18:57Z","pushed_at":"2019-11-06T14:40:59Z","stargazers_count":306,"language":"Python"},{"full_name":"feedly/transfer-nlp","description":"NLP library designed for reproducible experimentation management","topics":["nlp","transfer-learning","framework","playground","natural-language-understanding","language-model","pytorch"],"created_at":"2019-03-12T20:00:31Z","pushed_at":"2020-05-28T17:32:42Z","stargazers_count":290,"language":"Python"},{"full_name":"L0SG/relational-rnn-pytorch","description":"An implementation of DeepMind's Relational Recurrent Neural Networks (NeurIPS 2018) in PyTorch.","topics":["pytorch","language-model","word-language-model","language-modeling","deep-learning","recurrent-neural-networks","deepmind","transformer","self-attention"],"created_at":"2018-08-21T07:57:41Z","pushed_at":"2018-12-27T05:38:23Z","stargazers_count":243,"language":"Python"},{"full_name":"backprop-ai/backprop","description":"Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.","topics":["natural-language-processing","nlp","question-answering","bert","language-model","text-classification","multilingual-models","image-classification","fine-tuning","transfer-learning"],"created_at":"2020-10-30T15:25:14Z","pushed_at":"2021-05-03T09:15:25Z","stargazers_count":240,"language":"Python"},{"full_name":"rylans/getlang","description":"Natural language detection package in pure Go","topics":["nlp","natural-language","language-model"],"created_at":"2018-03-01T21:27:30Z","pushed_at":"2020-12-27T07:47:21Z","stargazers_count":162,"language":"Go"},{"full_name":"rdspring1/PyTorch_GBW_LM","description":"PyTorch Language Model for 1-Billion Word (LM1B / GBW) Dataset","topics":["pytorch","language-model","lstm","deep-learning","gpu","machine-learning","nlp","torch","torch-gbw"],"created_at":"2017-11-15T14:54:57Z","pushed_at":"2019-08-22T22:08:16Z","stargazers_count":121,"language":"Python"},{"full_name":"batzner/tensorlm","description":"Wrapper library for text generation / language models at char and word level with RNN in TensorFlow","topics":["language-model","tensorflow","tensorflow-library","char-rnn","char-lm","nlp"],"created_at":"2017-08-20T12:54:14Z","pushed_at":"2022-06-21T21:07:11Z","stargazers_count":61,"language":"Python"}],"frecuent_topics":{"language-model":27,"pytorch":14,"nlp":9,"deep-learning":9,"bert":4}}