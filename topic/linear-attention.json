{"category_type":"topic","category_name":"linear-attention","repos_data":[{"full_name":"BlinkDL/RWKV-LM","description":"RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.","topics":["attention-mechanism","deep-learning","gpt","gpt-2","gpt-3","language-model","linear-attention","lstm","pytorch","rnn"],"created_at":"2021-08-08T06:05:27Z","pushed_at":"2023-12-28T06:53:30Z","stargazers_count":10645,"language":"Python"}],"frecuent_topics":{"attention-mechanism":1,"deep-learning":1,"gpt":1,"gpt-2":1,"gpt-3":1}}