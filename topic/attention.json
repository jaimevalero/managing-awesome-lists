{"category_type":"topic","category_name":"attention","repos_data":[{"full_name":"jadore801120/attention-is-all-you-need-pytorch","description":"A PyTorch implementation of the Transformer model in \"Attention is All You Need\".","topics":["attention","deep-learning","attention-is-all-you-need","pytorch","nlp","natural-language-processing"],"created_at":"2017-06-14T10:15:20Z","pushed_at":"2023-10-05T02:14:06Z","stargazers_count":8173,"language":"Python"},{"full_name":"szagoruyko/attention-transfer","description":"Improving Convolutional Networks via Attention Transfer (ICLR 2017)","topics":["pytorch","knowledge-distillation","attention","deep-learning"],"created_at":"2017-01-17T15:38:09Z","pushed_at":"2018-07-11T11:49:59Z","stargazers_count":1419,"language":"Python"},{"full_name":"jnhwkim/ban-vqa","description":"Bilinear attention networks for visual question answering","topics":["visual-question-answering","attention","bilinear-pooling","pytorch-implmention"],"created_at":"2018-06-12T13:10:02Z","pushed_at":"2023-10-30T00:48:04Z","stargazers_count":529,"language":"Python"},{"full_name":"kaushalshetty/Structured-Self-Attention","description":"A Structured Self-attentive Sentence Embedding","topics":["attention-mechanism","attention-model","self-attention","self-attentive-rnn","pytorch","deep-learning","python3","attention","visualization","classification"],"created_at":"2018-02-01T07:38:55Z","pushed_at":"2019-09-22T21:00:24Z","stargazers_count":494,"language":"Python"},{"full_name":"benedekrozemberczki/AttentionWalk","description":"A PyTorch Implementation of \"Watch Your Step: Learning Node Embeddings via Graph Attention\" (NeurIPS 2018).","topics":["pytorch","torch","attention","deepwalk","node2vec","word2vec","matrix-factorization","deep-learning","structural-attention","walklet"],"created_at":"2019-01-11T15:09:12Z","pushed_at":"2022-11-06T21:09:53Z","stargazers_count":310,"language":"Python"},{"full_name":"benedekrozemberczki/GAM","description":"A PyTorch implementation of \"Graph Classification Using Structural Attention\" (KDD 2018).","topics":["graph-convolution","gcn","graph-attention","attention","structural-attention","deepwalk","node2vec","graph-classification","graph2vec","graphsage"],"created_at":"2018-12-28T10:05:15Z","pushed_at":"2022-11-06T21:14:04Z","stargazers_count":263,"language":"Python"}],"frecuent_topics":{"attention":6,"deep-learning":4,"pytorch":4,"deepwalk":2,"node2vec":2}}