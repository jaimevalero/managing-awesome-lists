{"category_type":"topic","category_name":"knowledge-distillation","repos_data":[{"full_name":"huawei-noah/Pretrained-Language-Model","description":"Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.","topics":["knowledge-distillation","model-compression","quantization","pretrained-models","large-scale-distributed"],"created_at":"2019-12-02T14:26:04Z","pushed_at":"2023-05-21T13:34:36Z","stargazers_count":2910,"language":"Dockerfile"},{"full_name":"szagoruyko/attention-transfer","description":"Improving Convolutional Networks via Attention Transfer (ICLR 2017)","topics":["pytorch","knowledge-distillation","attention","deep-learning"],"created_at":"2017-01-17T15:38:09Z","pushed_at":"2018-07-11T11:49:59Z","stargazers_count":1419,"language":"Python"}],"frecuent_topics":{"knowledge-distillation":2,"model-compression":1,"quantization":1,"pretrained-models":1,"large-scale-distributed":1}}