{"category_type":"topic","category_name":"quantization","repos_data":[{"full_name":"huawei-noah/Pretrained-Language-Model","description":"Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.","topics":["knowledge-distillation","model-compression","quantization","pretrained-models","large-scale-distributed"],"created_at":"2019-12-02T14:26:04Z","pushed_at":"2023-05-21T13:34:36Z","stargazers_count":2910,"language":"Dockerfile"},{"full_name":"aaron-xichen/pytorch-playground","description":"Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)","topics":["pytorch-tutorial","pytorch-tutorials","pytorch","quantization"],"created_at":"2017-04-28T06:22:30Z","pushed_at":"2022-11-22T04:06:41Z","stargazers_count":2522,"language":"Python"},{"full_name":"google/qkeras","description":"QKeras: a quantization deep learning library for Tensorflow Keras","topics":["deep-learning","quantization","quantized-neural-networks","hardware-acceleration","quantized-networks","tensorflow","keras","machine-learning","asic-design","fpga"],"created_at":"2019-08-06T22:11:58Z","pushed_at":"2023-11-04T00:45:47Z","stargazers_count":514,"language":"Python"}],"frecuent_topics":{"quantization":3,"knowledge-distillation":1,"model-compression":1,"pretrained-models":1,"large-scale-distributed":1}}