{"category_type":"topic","category_name":"instruction-following","repos_data":[{"full_name":"tatsu-lab/stanford_alpaca","description":"Code and documentation to train Stanford's Alpaca models, and generate the data.","topics":["deep-learning","instruction-following","language-model"],"created_at":"2023-03-10T23:33:09Z","pushed_at":"2023-06-07T06:54:43Z","stargazers_count":28052,"language":"Python"},{"full_name":"zjunlp/KnowLM","description":"An Open-sourced Knowledgable Large Language Model Framework.","topics":["llama","large-language-models","pre-trained-language-models","language-model","instruction-following","deep-learning","chinese","english","instructions","models"],"created_at":"2023-04-01T03:45:31Z","pushed_at":"2024-01-19T05:20:33Z","stargazers_count":866,"language":"Python"},{"full_name":"SinclairCoder/Instruction-Tuning-Papers","description":"Reading list of Instruction-tuning. A trend starts from Natrural-Instruction (ACL 2022), FLAN (ICLR 2022) and T0 (ICLR 2022).","topics":["cross-task-generalization","generalization","instruction","multi-task-learning","natural-language-processing","natural-language-generation","natural-language-understanding","prompt-learning","instruction-following","large-language-models"],"created_at":"2022-11-24T06:26:08Z","pushed_at":"2023-07-20T02:31:08Z","stargazers_count":699,"language":"unknown"}],"frecuent_topics":{"instruction-following":3,"deep-learning":2,"language-model":2,"large-language-models":2,"llama":1}}